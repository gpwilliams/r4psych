# Advanced Statistical Tests

In this session, we'll cover more advanced statistical tests, such as multifactorial experiments and experiments with factors of more than 2 levels. 

Specifically, we'll cover:

- multilevel and multifactorial ANOVA
- multilevel and multiple regression
- factor coding (and why it matters in R)
- sum of squares (and why this matters in R)

As with the previous section, this section assumes some familiarity with basic statistics. It is beyond the score of this section to teach the basics behind all of these methods, but we will cover some background on how these methods work, and why you might choose to use them in different scenarios.

## Getting Started 

As always, we first need to load the `tidyverse` set of packages for this Chapter.

```{r 7_load_packages, include = FALSE}
library(tidyverse)
```

```{r 7_load_packages_class, eval = FALSE}
library(tidyverse)
```

## Multilevel Analyses

Often, your experiments will have more than one level for a factor. However, how you code these factors determines the answers that you get from your analyses.

Take for example evaluating the efficacy of a host of drugs. We often want to compare performance of several drugs against a placebo. In this case, we might perform an analysis where we compare different levels of one factor (drug; with placebo, drug one, and drug two levels) against a reference level (drug level one; placebo). In this case, we would use **treatment** coding for our factors, as we only want to know if drug one and drug two perform differently from the placebo. That is, we are interested in the **simple effects** of the drug.

However, in other instances we might want to detect a main effect of a factor to see if there is an overall effect of condition, and you want to explore which conditions differ after the fact. In this case, we might want to use **deviation** or **sum** coding for our factors, and as such we want to test for the **main effect** of condition.

To explore main effects for the ANOVA and for the multilevel regression, we'll simulate some data. We'll use a similar (but more complex) example from the previous lesson.

We'll simulate data for a numerical stroop task, where subjects need to pick the highest value number from two numbers. Crucially, these numbers can be displayed such that the highest number is biggest in font size (congruent), smallest in font size (incongruent), or the two numbers have their fonts at an equal size (neutral). Our hypothesis is that participants will identify the larger number most quickly if the size and magnitude of the number of congruent, slower if they are neutral, and slowest if they are incongruent.

We'll generate 60 subjects, with the congruent reaction time being around 400ms, the incongruent reaction time being on average 60ms slower than this, and the neutral condition around 30ms slower than the congruent condition (and thus around 30ms faster than the neutral condition).

Finally, we'll gather the data together, assuming you'll often have your data in a long format.

```{r 7_generate_data_multilevel_ANOVA_one}
set.seed(1000)
stroop_dat <- tibble(
  subject = seq(1:60),
  congruent = rnorm(mean = 400, sd = 30, n = 60),
  incongruent = congruent + rnorm(mean = 60, sd = 10, n = 60),
  neutral = congruent + rnorm(mean = 30, sd = 10, n = 60)
  ) %>%
  gather(key = cond, value = reaction_time, 2:4) %>% 
  mutate(
    cond = as.factor(cond),
    log_RT = log(reaction_time)
  )
```

To explore simple effects, we'll create some data that looks at the effects of a drug intervention on blood oxygen levels (outcome) for smokers. We have a control condition (placebo) where the oxygen level is on average 80mm Hg, "drug_one", which on average should be no different to the placebo, and "drug_two", which increases blood oxygen to around 100mm Hg. Here, we simply want to know if either of the drugs perform any better than the control condition. (Note, I have no idea if these are realistic values, but that's not the point of this exercise!)

```{r 7_simulate_data_multilevel_ANOVA_two}
# set parameters
set.seed(1000)
means <- rep(c(80, 80, 100), 100)
sds <- rep(c(5, 5, 7), 100)

# simulate data
cond <- rep(c("control", "drug_one", "drug_two"), 100)
subject <- seq(1:300)

smoking_dat <- tibble(
  subject = subject,
  cond = as.factor(cond),
  outcome = rnorm(n = 300, mean = means, sd = sds)
)
```

### Preparation of Data

#### Factors

In the above examples, we've created data sets where our condition column is stored as a factor. Take a look at this:

```{r 7_explore_data}
stroop_dat
smoking_dat
```

This is because I used `mutate()` to change the column from a character data type, to a factor data type. The nice thing about storing your factors this way is that we can see the levels that make up our factor:

```{r 7_explore_levels}
levels(stroop_dat$cond)
levels(smoking_dat$cond)
```

If your data isn't stored as a factor, you can convert it to one using `as.factor()` or `factor()` on your column (e.g. `data$factor <- as.factor(data$factor)`).

#### Contrast Matrices

We can see how we specify our contrasts for our tests by using the `contrasts()` function any any factors. 

Let's look back at the Stroop data when we only had two levels of condition, congruent and incongruent. Here, we'll subset the data and then use the `contrasts()` function to look at the contrast matrix. We'll also mutate the cond column to itself as a factor so we drop the level that is no longer present (i.e. neutral) ; this is just a little R trick!

```{r 7_filter_and_reset_levels_stroop}
# make data 2 levels and convert condition to a factor
stroop_subset <- stroop_dat %>% 
  filter(cond %in% c("congruent", "incongruent")) %>% 
  mutate(cond = factor(cond))

# see your contrasts
contrasts(stroop_subset$cond)
```

We can see from our contrast matrix that the congruent condition is assigned a value of 0, and the incongruent condition is assigned a value of 1. What this means is that our intercept in our linear model is the congruent condition, and we evaluate the effect of a shift in condition by 1 point (i.e. to the incongruent condition) against this reference level. Remember that in a linear model, the intercept is the point on the y-axis where x is 0. 

Let's fit a linear model to see how this works. Notice that we fit the model and then (`%>%`) use the `summary()` function to see the output of the model:

```{r 7_linear_model_stroop_subset}
lm(log_RT ~ cond, stroop_subset) %>% summary()
```

We can see that our parameter estimate for the intercept is 5.979, and being in the congruent condition adds 0.145 to the intercept value of 5.979, which sums to `r 5.979 + 0.145`. How does this equate to our mean scores?

```{r 7_stroop_subse_group_means}
stroop_subset %>% 
  group_by(cond) %>% 
  summarise(mean = mean(log_RT))
```

Great, our intercept corresponds to the mean of the congruent condition, and our effect of condition is the difference between the two means! That's because we used treatment coding, which is the default in R. This means that the intercept is alwasy the first alphabetic level of our conditions.

But what if we want to change the reference level of our intercept? This is important if we want to look at different contrasts when we have more than 1 level in a factor, or more than 2 factors in a model.

To change our intercept, we must change our contrast matrix. To change this we use the `contrasts()` function on our condition factor, to see what our contrasts are, and we can then define the contrasts we'd like using some inbuilt functions in R. Here, we'll change the coding to sum coding using `contr.sum`. (Others are available, such as `contr.helmert`, etc.)

```{r 7_sum_code_stroop_subset}
contrasts(stroop_subset$cond) <- contr.sum
contrasts(stroop_subset$cond)
```

Now, our contrast matrix sums to 0. Remember that our intercept is 0, so the intercept is now the mid-point between the 2 conditions, or the mean across both conditions. Let's refit our model and see this in action.

```{r 7_linear_model_recoded_stroop_subset}
lm(log_RT ~ cond, stroop_subset) %>% summary()
```

Now our intercept is 6.052, and the effect of condition (now labelled `cond1`; the first (and only) contrast in our contrast matrix for condition) is -0.073. Crucially, since we only have 2 levels of condition, our interpretation of all of the statistics in this model remains the same, only our intercept paramter estimate is different to before. Further, the parameter estimate for condition (and the standard error of this estimate) are half as large as before, but this is just due to the scale on which our factor is evaluated; the *t*- and *p*-values remain the same.

How does the intercept relate to our means? Check it out!

```{r 7_stroop_subset_overall_mean}
stroop_subset %>% summarise(mean = mean(log_RT))
```

The mean for both conditions is the intercept for our linear model. 

### Multilevel Regression

What happens if we have some more complex data? Let's look at the contrast matrix for the full Stroop data:

```{r 7_stroop_contrasts}
contrasts(stroop_dat$cond)
```

You can see that right now, the first row for the congruent condition sums to 0. Whereas the incongruent and neutral condition rows sum to 1. This means that the congruent condition will be the intercept in our model, and the incongruent and neutral conditions will be the other parameters in our model. Let's see this in action.

```{r 7_stroop_linear_model}
lm(log_RT ~ cond, data = stroop_dat) %>% summary
```

Again, we'll check how these parameters correspond to our means for each group. 

```{r 7_stroop_group_means}
stroop_dat %>% group_by(cond) %>% summarise(mean = mean(log_RT))
```

Great, the intercept is indeed the mean for the congruent condition, and the incongruent condition (cond2) and neutral (cond3) are the difference between the intercept mean and that reference level's mean.

Instead, we could get sum coded effects as before. 

```{r 7_stroop_contrasts_sum}
contrasts(stroop_dat$cond) <- contr.sum
contrasts(stroop_dat$cond)
```

Now the first 2 rows sum to 1, which together makes 2, and the final row sums to -2. Together, the intercept or 0 is the average across these contrasts. How does this affect our model?

```{r 7_stroop_linear_model_sum}
lm(log_RT ~ cond, data = stroop_dat) %>% summary
```

Now we can see that the intercept has changed, and now the congruent and incongruent condition are considered against the grand mean across all levels, so their terms have changed too.

```{r 7_stroop_grand_means}
stroop_dat %>% summarise(mean = mean(log_RT))
```

Check the following values against the group means above: 6.0516 - 0.073 = `r 6.051618 - 0.073005` and 6.0516 + 0.0728 = `r 6.051618 + 0.072803`, you can see that the group means for the congruent and incongruent condition equate to the intercept + the new paramter estimates.

Again, however, the main effect of our model remains unchanged regardless of the coding scheme used.

For now, we'll return to using treatment coding and we'll explore changing the order for the intercept. If we want to change the order, or determine exactly which level will be the intercept, we simply have to relevel our factor before defining our contrasts:

```{r 7_stroop_treatment}
contrasts(stroop_dat$cond) <- contr.treatment
stroop_dat$cond <- factor(stroop_dat$cond, levels = c("neutral", "congruent", "incongruent"))
```

Here, we simply asked R to remake our condition column from a factor of our condition column, but with specifically ordered levels.

Now when we specify our treatment contrasts, the reference level (i.e. 0) will be the first level that we specified above (neutral, in this instance).

```{r 7_contrasts_stroop_treatment}
contrasts(stroop_dat$cond)
```

We'll rerun the model to see how our intercept has changed.

```{r 7_stroop_linear_model_treatment}
lm(log_RT ~ cond, data = stroop_dat) %>% summary
```

This looks very similar to the grand mean, but notice the value of the parameter estimate beyond 3 decimal places. The paramter estimate is now 6.051819 which is the value of the mean for the neutral condition (while 6.051618 is the value of the grand mean across all conditions).

### Multilevel ANOVA

Let's say we're interested in main effects and we want to see if there is a main effect of condition in the numerical stroop task. Nicely, the `aov()` function defaults to give us main effects in the form of a traditional ANOVA output.

Let's see how this works.

```{r 7_multilevel_anova_model}
stroop_aov <- aov(log_RT ~ cond, data = stroop_dat)
summary(stroop_aov)
```

Now, our model corresponds to the model output that we got using the `lm()` function. Look at the bottom of the results from the linear model in the previous section, the F, df, and *p*-values are the same. This is because the `aov()` function is simply a wrapper for our linear model that gives us main effects! Under the hood, `aov()` is just `lm()`, only without the additional model coefficients from the linear model.

However, if we've already fitted a model using the `aov()` function, but we want to retrieve the model coefficients, we can use a different type of `summary()` function, `summary.lm()`. This makes R give us our model with a linear model output. 

```{r 7_summary_multilevel_anova_model}
summary.lm(stroop_aov)
```

Again, the fit from this model gives coefficients which correspond to those that we determine using the `contrasts()` function. 

This contrast coding will become more important as we fit more factors in our models.

### Multiple Contrasts

Let's say we have the smoking data set, and we're interested in whether two drugs differ from a placebo. We can fit the model with a treatment coded contrast matrix so that the intercept is the placebo (control group), and the other conditions are compared against this reference value. We'll fit this with a linear model, seeing as we care about the model coefficients.

```{r 7_smoking_linear_model_main_effects}
smoking_lm <- lm(outcome ~ cond, data = smoking_dat)
summary(smoking_lm)
```

As you can see, the two drug conditions are compared aginst the intercept for the placebo condition. However, what if we want to consider the effectiveness of the two drugs against one-another?

We have two options:

1. Relevel your factor so that you can compare drug_one to drug_two (i.e. set drug_one as the intercept)
2. Calculate the differences in the parameter estimates for the two rows in the table.

The first option is easiest if we only have a few levels in a factor. The second is handy if we have a lot of levels, but requires us to specify a complex contrast matrix. For the first option, we simply need to do something like this:

```{r 7_relevel_smoking_data}
smoking_dat$cond <- factor(smoking_dat$cond, levels = c("drug_one", "drug_two", "control"))
smoking_lm2 <- lm(outcome ~ cond, data = smoking_dat)
summary(smoking_lm2)
```

Now our intercept is drug_one, the parameter estimate for conddrug_two is the difference between drug_one and drug_two, and condcontrol corresponds to the parameter estimates for the difference between drug_one and the control condition. Notice that condcontrol here equates to conddrug_one in the original model.

For more complex cases (i.e. several levels) there are helper packages such as `multcomp` which allows you to specify a contrast matrix and conduct multiple comparisons on your model, but we won't get into that here.

## Multifactorial Analyses

Now we'll look at cases where we have more than one factor. Here, we'll use one of R's inbuilt data sets, ToothGrowth, which looks at the effect of vitamin C on tooth growth in guinea pigs. This data set has 3 columns:

- len: tooth length
- supp: supplement type (vitamin C supplement or orange juice)
- dose: strength of dose

Most importantly, supplement is a factor with 2 levels, and dose is a numeric variable with 3 levels, but the same principles apply to more complex designs.

Load the data, and save it as a tibble with the columns saved as factors:

```{r 7_toothgrowth_data}
# load the data
data("ToothGrowth")

# convert to tibble and make dose a factor
tooth <- ToothGrowth %>%
  mutate(dose = factor(dose)) %>%
  as.tibble()

# see the output
tooth
```

### Multifactorial ANOVA

We'll conduct an ANOVA to check for a main effect of each factor, and the interaction between them. You already know how to check for main effects, and we can simply check for more main effects by adding them to the formula like so:

`dependent variable ~ factor_one + factor_two`

To check for an interaction, we specify it like so:

`dependent variable ~ factor_one + factor_two + factor_one : factor_two`

Or, to save on typing, like so (these two are equivalent):

`dependent variable ~ factor_one * factor_two`

First, we'll check how our factors are instantiated (i.e. what are the contrasts)?

```{r 7_tooth_contrasts}
contrasts(tooth$supp) 
contrasts(tooth$dose)
```

Here, we're interested in the main effects of both factors, and whether there's an interaction between them. Since we aren't going to get the parameter estimates from this model using the `summary.lm()` function, we don't need to specify the contrast matrix required for the ANOVA here. 

We'll save this as **tooth_aov** as we want to do some follow up tests later on.

```{r 7_tooth_aov}
tooth_aov <- aov(len ~ supp * dose, data = tooth)
summary(tooth_aov)
```

As you can see, we have a significant main effect of both factors, and a significant interaction. As such, we need to do some follow up tests to see where our effect lies in the interaction. A quick check with a plot always helps:

```{r 7_tooth_plot}
ggplot(data = tooth, mapping = aes(x = dose, y = len, colour = supp)) +
  geom_boxplot()
```

It looks like there's a main effect of dose overall, but also that supplement type probably only matters for lower doses. Let's check these using some pairwise tests. As such, perform pairwise comparisons to see where our differences lie.

#### Multiple Comparisons

One option is to perform multiple comparisons using Tukey Honest Significant Difference. We do this by applying the `TukeyHSD()` function to our fitted model **tooth_aov**. Here, we also have to specify the confidence level, which is by default 95%.

```{r 7_tukey_comparisons}
TukeyHSD(tooth_aov, conf.level = 0.95)
```

We get a lot of output here corresponding to every possible comparison we could ever want. We can remedy this by saving the output of TukeyHSD to an object, and selecting only the relevant information, **supp:dose**. Notice that we put this name in backticks so we can use the special character `:` without R interpreting it as something else.

```{r 7_tukey_comparisons_subset}
tukey_aov <- TukeyHSD(tooth_aov, conf.level = 0.95)
tukey_aov$`supp:dose`
```

### Multiple Regression

#### Three-Level Factors

If you want to see whether there's a main effect or interaction for 3 level factors, the easiest way is to simply run an ANOVA. With the `lm()` function, comparisons for the contribution of each factor in our model starts to become more difficult when we have factors with more than 2 levels. That's because we have to construct some numeric variables for these factors, and we need to perform model comparisons on a full model (with interactions) and reduced models (without) to see which model best fits our data. 

To do this, we simply have to centre our two factors, with these stored as numeric variables.

```{r 7_deviation_coding}
# code new centered variables
tooth$supp_dev <- (tooth$supp == "VC") - mean(tooth$supp == "VC")
tooth$dose_dev_one <- (tooth$dose == "0.5") - mean(tooth$dose == "0.5")
tooth$dose_dev_two <- (tooth$dose == "1") - mean(tooth$dose == "1")

# inspect changes
head(tooth)
```

This centering works similarly to the contrast matrices we used before. In fact, it is often the better option if we have unbalanced data sets, as it accounts for the mismatch in the number of observations in each level of a factor. The centering used in this case is called **deviation coding** and is just like sum coding, only the parameter estimates and standard errors are half as large (but all other interpretations, e.g. *t*- and *p*-values remain the same).

Now, when we fit a model with **supp_dev**, the intercept will be the mean of the two conditions. 

When it comes to fitting a 3 level factor, we need to do the same thing for only two of the levels. When we then add this to a model and test for an interaction, we test for the interaction for **supp_dev** across both columns for our deviation coded doses.

```{r 7_linear_model_deviation_full}
tooth_lm_full <- lm(len ~ supp_dev * (dose_dev_one + dose_dev_two), data = tooth)
```

Next, we construct a reduced model, and use the `anova()` function to compare the two models against one another.

```{r 7_linear_model_deviation_reduced}
tooth_lm_reduced <- lm(len ~ supp_dev + (dose_dev_one + dose_dev_two), data = tooth)
anova(tooth_lm_full, tooth_lm_reduced)
```

This checks for how much variance is explainedby each model. As you can see, the interaction term in our model makes a significant contribution to the variance explained, so this indicates that there's an interaction in our model, and as such we should explore this. 

A quick and easy way to do so is to run a number of *t*-tests or linear models on subsets of the data, adjusting for the *p*-values where necessary.

One way to achieve this for multifactorial designs is to create a new column which is a combination of the two factors.

```{r 7_create_interaction_column}
# create combined factor column
tooth$interact <- interaction(tooth$supp, tooth$dose)

# check levels of our new factor
levels(tooth$interact)
```

Then we simply use the `pairwise.t.test()` function, and supply it our dependent variable and our grouping factor. This method defaults to Holm's sequential bonerroni, which is like bonferroni adjustment of *p*-values, but rather than multiplying them all by the total number of comparisons, we instead multiply them incrementally, so the largest *p*-value is multiplied by 1, the next smallest by 2, the next smallest by 3, etc.

```{r 7_pairwise_t_tests}
pairwise.t.test(tooth$len, tooth$interact)
```

There are other ways to automate *t*-tests in R. For example, you could create a function to subset your data, apply a test, and adjust the *p*-values manually, but this is not necessary here.

#### Two-Level Factors

Things are a lot easier when we have factors with only 2 levels. Let's assume our data had only 2 factors with 2 levels each. How would we test for main effects and interactions with a linear model? 

To explore this question, we'll subset our data to only 2 levels for dose, and we'll keep only the original columns. (We use `dplyr::select` to be specific that we want select from the `dplyr` package, not another loaded package.). Remember, we want to recreate dose as a factor again so it doesn't remember our old level.

```{r 7_subset_tooth}
tooth_sub <- tooth %>% 
  dplyr::select(1 : 3) %>%
  filter(dose %in% c(0.5, 1)) %>%
  mutate(dose = factor(dose))
```

Now, we should check our contrasts as usual.

```{r 7_tooth_contrasts_again}
contrasts(tooth_sub$supp)
contrasts(tooth_sub$dose)
```

We should make these sum coded contrasts so we can then get main effects from our model.

```{r 7_tooth_contrasts_to_sum}
contrasts(tooth_sub$supp) <- contr.sum
contrasts(tooth_sub$dose) <- contr.sum
```

Then we can fit the model again.

```{r 7_refit_tooth_model}
tooth_sub_lm <- lm(len ~ supp * dose, data = tooth_sub)
summary(tooth_sub_lm)
```

The only thing to note here is that we've used sum coding other than deviation coding with the 3-level case. Our interpretation of effects stays the same here, only our paramter estimates and standard errors will be twice as large with sum coding when compared to deviation coding.

If your factors are not sum/deviation coded, then you won't observe main effects. So we can test for the effect of dose at the first level of supplement as follows:

```{r 7_tooth_treatment_contrasts}
contrasts(tooth_sub$supp) <- contr.treatment
contrasts(tooth_sub$supp)
```

OJ will now be our reference value for the intercept. Additionally, the effect of dose will be compared at this intercept value, so we now have simple effects of dose, and main effects of supplement. The interpretation for our interaction terms remains the same regardless of our coding approach.

```{r 7_tooth_linear_model_treatment}
tooth_sub_lm2 <- lm(len ~ supp * dose, data = tooth_sub)
summary(tooth_sub_lm2)
```

## Mixed Analyses

Mixed factorial analyses are just a special form of factorial analyses. Here, one factor is between subjects and another factor is within subjects. Here, we'll create some new data based on our smoking data that reflects a mixed design.

```{r 7_simulate_data_for_mixed_analysis}
# set parameters
set.seed(1000)
means <- rep(c(80, 85), 100)
sds <- rep(c(5, 5, 7), 100)
cond <- rep(c("control", "drug_one"), 100)
subject <- seq(1:200)

# simulate data
smoking_dat_extra <- tibble(
  subject = subject,
  cond = as.factor(cond),
  time_one = rnorm(n = 200, mean = means, sd = sds),
  time_two = time_one + rnorm(n = 200, mean = 5, sd = sds)
  ) %>%
  gather(key = test_time,
         value = recovery,
         3:4
         ) %>%
  mutate(test_time = as.factor(test_time)) %>%
  arrange(subject)

# view the data
smoking_dat_extra
```

Now our smoking data is a little more complex, and a simpler in parts. For one, we now only have two drugs, the placebo (control) or drug_one. However now we've tested people at two time points to see if time plays a factor in their recovery.

### Mixed Linear Models

First, we'll make sure our factors are sum coded so that we can obtain main effects of both factors.

```{r 7_sum_code_factors_mixed_analysis}
contrasts(smoking_dat_extra$cond) <- contr.sum
contrasts(smoking_dat_extra$test_time) <- contr.sum
```

Then, we'll refit the model as before.

```{r 7_refit_mixed_linear_model}
mixed_lm <- lm(recovery ~ cond * test_time, 
                 data = smoking_dat_extra)
summary(mixed_lm)
```

Now our intercept corresponds to the grand mean, and our effects of condition and test time are main effects of these two factors. The interaction again tells us that we need to explore the data. For this, we'd just use the same methods from above.

However, this model doesn't account for the dependencies between subjects and their scores. In order to do this, we should use the 'aov()' function, which is simply a wrapper for the linear model, but allows for accounting for between-subjects factors. Alternatively, we can improve on our linear models by making them account for subjects-based effects, and we'll cover this in later sessions on **mixed effects modelling**.

### Mixed ANOVA

With a mixed ANOVA, we are simply just including an error term for our within-subjects factor. We do this in the same way as for within-subjects ANOVAs, only leaving the error term off the between-subjects factor.

In this case, subjects take part in both test time conditions, so we need to nest this within our data.

```{r 7_mixed_linear_anova_model}
mixed_aov <- aov(recovery ~ cond * test_time + 
                   Error(subject/test_time), 
                 data = smoking_dat_extra)
summary(mixed_aov)
```

Now we have our main effects of condition, test time, and the interaction between them. 

## A Note on Sum of Squares

In some cases, you might notice that the output of your ANOVA models does not correspond to the output of that from SPSS or other statistical packages. That's because R defaults to type-I sum of squares, and SPSS defaults to type-III sum of squares. 

Put simply, when you have an unbalanced design, such as when you have an unequal number of participants in each group, the way in which the sum of squares is calculated will affect the output and interpretation of your ANOVA. 

With type-I sum of squares, which is R's default, the order in which you specify factors in your model will determine how much variance is explained by each factor. With type-I sum of squares, the terms are considered in the model in a sequential order. Take a look at this in action below. We'll use the **tooth_sub** data from before, but we'll remove the first observation.

```{r 7_remove_data_tooth}
tooth_sub_unbalanced <- tooth_sub[2:40,]
```


```{r 7_supp_first_aov}
supp_first <- aov(len ~ supp * dose, data = tooth_sub_unbalanced)
summary(supp_first)
```

```{r 7_supp_second_aov}
dose_first <- aov(len ~ dose * supp, data = tooth_sub_unbalanced)
summary(dose_first)
```

Here, you can see that the sum of squares, and consequently the F-values (and hence *p*-values) differ depending on the order in which we entered our factors in the model. What happens is that the first term in the model is evaluated first, and any variance explained that is shared by dose and supp is instead given to the first term in the model. Therefore, we shouldn't really use type-I sum of squares unless our two factors are entirely independent of one-another.

Alternatively, we have type-II sum of squares, which evaluates the main effects of your factors while ignoring the variance explained by interaction terms. Crucially, if you do have an interaction, type-II sum of squares will not accurately reflect the variance explained by the interaction as that variance is given over to the main effect terms. Crucially, the coding scheme used for type-II sum of squares does not matter.

In Psychology, we typically use type-III sum of squares, where main effects are computed while taking the interaction into account. This method is recommended for unbalanced designs, although there is some debate on this. However, type-III sum of squares only work with orthogonal contrast matrices (i.e. sum/helmert coding), so make sure you pay attention to how you've coded your factors prior to fitting a model.

The way to refit our model with type-III sum of squares is to use the `Anova()` (notice the capital A) function from the **car** package, specifying the type for your sum of squares. This function takes the object of your model fitted by `aov()` and the type for the sum of squares you would like to compute. Note that this function defaults to type-II sum of squares, so we need to be specific in our function call.

First, install this package and load it, then run the function on the two model fits from above.

```{r 7_install_load_car}
# install.packages("car") # install once per machine
library(car) # load each time you open R
```


```{r 7_type_III_sum_of_squares}
Anova(supp_first, type = "III")
Anova(dose_first, type = "III")
```

As you can see, the results of both analyses are equivalent for the unbalanced data set using this function.

As a final note, in all cases we should check that the tests we run meet the assumptions of said test. For factorial ANOVAs, this means checking the previous assumptions for a one-way ANOVA, while also checking for a lack of multicollinearity between the factors in the design, which we won't cover here. 

## Exercises

For these exercises, we will look at the core concepts from this lesson. Here, we'll use a simulated data set that's saved as a .csv in the inputs folder.

If you don't have access to these, please [download the repository from GitHub](https://github.com/gpwilliams/r4psych) and open the **lesson_materials** folder. Open the relevant folder for this lesson. If you work out of the file **07_advanced_statistical_tests.Rmd** the code below will work to load the data set. If this fails then quit R and open it up from the .Rmd file above so your working directory is in the correct folder.

```{r 7_exercise_libraries, eval = FALSE}
library(tidyverse)
# install.packages("car") # install if necessary (once per machine)
library(car)
library(ggpirate)
```

Next, we'll load the data set for this exercise.

```{r 7_exercise_data, eval = FALSE}
factorial_data <- read_csv("inputs/factorial_data.csv")
```

### Question 1

Aggregate the data by subject such that you have 1 score per subject for the mean of the column Y across both A and B columns. Call the Y column mean_Y. Assign this to the object **data_agg**.

### Question 2

Generate summary statistics for the data set and a pirate plot of the data set. We want means, SDs, and ns for the table of summary statistics.

### Question 3

Define your contrasts such that you have main effects and interactions from any model fits. You may need to check your data types for this to work properly. 
*Hint*: use `mutate()` after the `ungroup()` function

### Question 4

Fit the data set checking for all main effects and interactions.

### Question 5

Refit your model from Question 5 using type-III sum of squares. Did that make any difference to your sum of squares? Why or why not?

### Question 6

Explore the parameter estimates from your ANOVA model, and briefly report your findings in text (1 sentence max).
